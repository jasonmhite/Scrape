#!/usr/bin/python
import feedparser
import urllib2
import ConfigParser
import string
import os

defaults = {"zeropaddedq" : "true"}

def build_searchtable(configfile):
    table = []
    config = ConfigParser.SafeConfigParser(defaults)
    config.read(configfile)
    for sect in config.sections():
        searchstring = config.get(sect, 'sstring')
        episode = config.get(sect, 'currentepisode')
	url = config.get(sect, 'feedurl')
	feed = feedparser.parse(url)
        zeropaddedepnum = config.get(sect, "zeropaddedq")
        if zeropaddedepnum == 'true' and int(episode) < 10:
            episode = "0" + episode
        query = searchstring.replace("*EP", episode)
        table.append((sect,query,feed))
    return(table)

def scan_query(nib):
    res = False
    for entry in nib[2].entries:
        title = filter(lambda x: x in string.printable, entry.title)
        if nib[1] in title:
            res = (nib[0], nib[1], entry.link)
	    return(res)
    return(res)

def scan_searchtable(searchtable):
    hits = []
    for entry in searchtable:
        hit = scan_query(entry)
	if hit:
	    hits.append(hit)
    return(hits)

def update(matchlist, configfile):
    newconfig = ConfigParser.SafeConfigParser()
    newconfig.read(configfile)
    for m in matchlist:
        i = int(newconfig.get(m[0], "currentepisode")) + 1
        newconfig.set(m[0], "currentepisode", str(i))
    with open(configfile, "wb") as updated:
        newconfig.write(updated)

def download(matches):
    for entry in matches:
        dl = urllib2.urlopen(entry[ 2 ], timeout=2)
        with open("/Users/jmhite/Automatically Downloaded/" + entry[1] + '.torrent', 'w') as outfile:
            outfile.write(dl.read())
        #os.system("/usr/local/bin/growlnotify -s -t Scrape -i torrent -m 'Downloading " + str(entry[1]) + "'" )

tab = build_searchtable("list.cfg")
result = scan_searchtable(tab)
download(result)
update(result, "list.cfg")

